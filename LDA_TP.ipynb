{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOT5N1tOPcC5UrLaLokz3P9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QHugT6ClTEe",
        "outputId": "51fa43c8-75d3-4fd5-eade-afeeae1cdb04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=d4b88d9df04dbb8f81dd539c0c09ad3cea0913e8b62559ff96fc907973cc3c02\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzZaG5G9hg7n",
        "outputId": "e456a4bf-c026-49f6-c790-6599cde51409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wikipedia\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "stemmer = WordNetLemmatizer()\n",
        "Shakti = wikipedia.page('Avengers (comics)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liwphkVuhxlB",
        "outputId": "32956252-b76c-45df-becc-f758ad9115a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(docu):\n",
        "    document = re.sub(r'[^a-zA-Z0-9 ]','',str(docu))\n",
        "    tokens = document.split()\n",
        "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "    tokens = [word for word in tokens if word not in en_stop]\n",
        "    tokens = [word for word in tokens if len(word)>5]\n",
        "\n",
        "    return tokens\n",
        "preprocess_data = [];\n",
        "tokens = preprocess_text(Shakti)\n",
        "preprocess_data.append(tokens)"
      ],
      "metadata": {
        "id": "5PzD16HJh-kw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dict = corpora.Dictionary(preprocess_data)\n",
        "input_corpora = [input_dict.doc2bow(token, allow_update=True) for token in preprocess_data]"
      ],
      "metadata": {
        "id": "HtQwzGo-n3yc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(input_corpora,num_topics=5,id2word = input_dict,passes=10)\n",
        "topics = lda_model.print_topics(num_words=2)\n",
        "print(topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbnK--bBiDJG",
        "outputId": "eb371c07-208c-42fe-ca8e-7318e12d4f4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.500*\"Avengers\" + 0.500*\"WikipediaPage\"'), (1, '0.500*\"WikipediaPage\" + 0.500*\"Avengers\"'), (2, '0.500*\"Avengers\" + 0.500*\"WikipediaPage\"'), (3, '0.500*\"Avengers\" + 0.500*\"WikipediaPage\"'), (4, '0.500*\"WikipediaPage\" + 0.500*\"Avengers\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocess_data, dictionary=input_dict, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf2kSy1olCGe",
        "outputId": "8949e35e-65d3-4b46-8a3d-75842a4a8f3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score:  0.9999999999999998\n"
          ]
        }
      ]
    }
  ]
}